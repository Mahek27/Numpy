{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from helpers import *\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_SRC = \"C:/Users/Mahek/Desktop/premier-league\"\n",
    "df = pd.read_csv(DATA_SRC)\n",
    "\n",
    "# create win/lose label\n",
    "df['target'] = df[['Score_home', 'Score_away']].apply(score_to_win, axis = 1)\n",
    "df.sort_values('MatchID', inplace = True)\n",
    "df.head()\n",
    "\n",
    "#Feature Extraction\n",
    "df_wo = df.drop(columns = ['target', 'MatchID', 'Home_team', 'Away_team', 'Score_home', 'Score_away', 'year'])\n",
    "print(len(df_wo.columns))\n",
    "list(df_wo)\n",
    "\n",
    "#correlation\n",
    "import seaborn as sns\n",
    "home_features = [ f for f in list(df_wo) if '_home' in f ]\n",
    "corr = df_wo[home_features].corr()\n",
    "_ = sns.heatmap(corr)\n",
    "\n",
    "#Feature Engineering\n",
    "gd = gd_vectors(scores)\n",
    "\n",
    "away_form_linear = []\n",
    "home_form_linear = []\n",
    "away_form_exp = []\n",
    "home_form_exp = []\n",
    "for game in scores:\n",
    "    id, home_team, away_team, _, _ = game\n",
    "    away_form_exp.append( exponential_momentum(id, away_team, gd, alpha = .65) )\n",
    "    home_form_exp.append( exponential_momentum(id, home_team, gd, alpha = .65) )\n",
    "    away_form_linear.append( linear_momentum(id, away_team, gd) )\n",
    "    home_form_linear.append( linear_momentum(id, home_team, gd) )\n",
    "\n",
    "\n",
    "df_form = df.copy()\n",
    "df_form['away_form_exp'] = pd.Series(away_form_exp)\n",
    "df_form['home_form_exp'] = pd.Series(home_form_exp)\n",
    "list(df_form)\n",
    "\n",
    "#Modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "clfs = [LogisticRegression(), RandomForestClassifier(), GradientBoostingClassifier(),\n",
    "        KNeighborsClassifier()]\n",
    "df_form.drop(columns = ['target', 'MatchID', 'Home_team', 'Away_team', \n",
    "                                 'Score_home', 'Score_away', 'year'], inplace = True)\n",
    "X = df_wo.values\n",
    "X_form = df_form.values\n",
    "y = df['target'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 42)\n",
    "X_train_form, X_test_form, y_train_form, y_test_form = train_test_split(X_form, y, test_size = .2, random_state = 42)\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "X_train_form_std = sc.fit_transform(X_train_form)\n",
    "X_test_form_std = sc.transform(X_test_form)\n",
    "\n",
    "for clf in clfs:\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(type(clf))\n",
    "    print(\"score = \", clf.score(X_test, y_test), \"\\n\")\n",
    "    \n",
    "#With scaled variables\n",
    "for clf in clfs:\n",
    "    clf.fit(X_train_std, y_train)\n",
    "    print(type(clf))\n",
    "    print(\"score = \", clf.score(X_test_std, y_test), \"\\n\")\n",
    "    \n",
    "for clf in clfs:\n",
    "    clf.fit(X_train_form_std, y_train_form)\n",
    "    print(type(clf))\n",
    "    print(\"score = \", clf.score(X_test_form_std, y_test_form), \"\\n\")\n",
    "    \n",
    "#Coefficient Investigation\n",
    "for i, feature in enumerate(list(df_form)):\n",
    "    print(feature, \": \", clfs[0].coef_[:,i])\n",
    "features_to_drop = ['Shots_home', 'Shots_away', 'Touches_home', \n",
    "                                 'Touches_away', 'Possession_home', 'Possession_away',\n",
    "                                 'Tackles_home', 'Tackles_away', 'Arrivals_home', 'Arrivals_away',\n",
    "                                 'Departures_home', 'Departures_away', 'Corners_home', 'Corners_away',\n",
    "                                 'Red_cards_home', 'Red_cards_away', 'Yellow_cards_home', 'Yellow_cards_away']\n",
    "df_sub = df_form.drop(columns = features_to_drop)\n",
    "print(list(df_sub))\n",
    "\n",
    "X_sub = df_sub.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sub, y, test_size = .2, random_state = 42)\n",
    "\n",
    "for clf in clfs:\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(type(clf))\n",
    "    print(\"score = \", clf.score(X_test, y_test), \"\\n\")\n",
    "    \n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(df_sub.values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = .2, random_state = 42)\n",
    "\n",
    "for clf in clfs:\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(type(clf))\n",
    "    print(\"score = \", clf.score(X_test, y_test), \"\\n\")\n",
    "    \n",
    "#Assesing feature importance with random forest\n",
    "forest = RandomForestClassifier(n_estimators=500,random_state=42)\n",
    "forest.fit(X_train_form, y_train)\n",
    "\n",
    "features = df_form.columns\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "indices\n",
    "for f in range(X_train_form.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f + 1, 30,features[indices[f]], importances[indices[f]]))\n",
    "    \n",
    "#Feature selection with backward selection\n",
    "from SBS import *\n",
    "lr = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "gb = GradientBoostingClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "clf_labels = ['Logistic Regression', 'Random Forest', 'Gradient Boosting', 'KNN']\n",
    "\n",
    "all_clf = [lr, rf, gb, knn]\n",
    "\n",
    "for label, clf in zip(clf_labels, all_clf):\n",
    "    print(label, clf)\n",
    "    \n",
    "k_feat = {key: None for key in clf_labels}\n",
    "sbs = {key: None for key in clf_labels}\n",
    "\n",
    "sbs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for label, clf in zip(clf_labels, all_clf):\n",
    "    sbs[label] = SBS(clf,k_features=1)\n",
    "    sbs[label].fit(X_train_form,y_train)\n",
    "    k_feat[label] = [len(k) for k in sbs[label].subsets_]\n",
    "    plt.plot(k_feat[label], sbs[label].scores_, marker='o')\n",
    "    #plt.ylim([0.3, 1.02])\n",
    "    plt.title(label)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Number of features')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "#RESULT\n",
    "#Logistic regression\n",
    "lr_features = list(sbs['Logistic Regression'].subsets_[20])\n",
    "for idx, i in enumerate(lr_features):\n",
    "    print(idx,features[i])\n",
    "    \n",
    "#Rendom forest\n",
    "rf_features = list(sbs['Random Forest'].subsets_[13])\n",
    "for idx, i in enumerate(rf_features):\n",
    "    print(idx,features[i])\n",
    "    \n",
    "#Gradient boosting\n",
    "gb_features = list(sbs['Gradient Boosting'].subsets_[18])\n",
    "for idx, i in enumerate(gb_features):\n",
    "    print(idx,features[i])\n",
    "    \n",
    "#Checking performance of the selected feature\n",
    "for clf in clfs:\n",
    "    clf.fit(X_train_form, y_train_form)\n",
    "    print(type(clf))\n",
    "    print(\"score = \", clf.score(X_test_form, y_test_form), \"\\n\")\n",
    "    \n",
    "lr.fit(X_train_form[:, lr_features], y_train_form)\n",
    "print('Train accuracy:', lr.score(X_train_form[:, lr_features], y_train_form))\n",
    "print('Test accuracy:', lr.score(X_test_form[:, lr_features], y_test_form))\n",
    "\n",
    "rf.fit(X_train_form[:, rf_features], y_train_form)\n",
    "print('Train accuracy:', rf.score(X_train_form[:, rf_features], y_train_form))\n",
    "print('Test accuracy:', rf.score(X_test_form[:, rf_features], y_test_form))\n",
    "\n",
    "gb.fit(X_train_form[:, gb_features], y_train_form)\n",
    "print('Train accuracy:', gb.score(X_train_form[:, gb_features], y_train_form))\n",
    "print('Test accuracy:', gb.score(X_test_form[:, gb_features], y_test_form))\n",
    "\n",
    "#Ensemble Model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "for label, clf in zip(clf_labels, all_clf):\n",
    "    scores = cross_val_score(estimator=clf, X = X_train_form, y = y_train_form, cv = 10, scoring = 'accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
